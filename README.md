# Realtime Speech Emotion Recognition using LSTM
This project implements a Realtime Speech Emotion Recognition (SER) system using Long Short-Term Memory (LSTM) networks. The goal is to classify emotions conveyed in human speech by analyzing audio signals in real-time.

The system processes audio inputs to extract meaningful features such as Mel-frequency Cepstral Coefficients (MFCCs), Zero-Crossing Rate (ZCR), and Root Mean Square Energy (RMSE). These features are then fed into an LSTM model, which captures the temporal patterns in the audio data to accurately predict emotions.
